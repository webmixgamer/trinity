# OWASP Top 10 for LLM Applications 2025

> **Source**: [OWASP LLM Top 10](https://genai.owasp.org/llm-top-10/)
> **Project**: [OWASP Top 10 for Large Language Model Applications](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
> **Last Updated**: December 2025

The OWASP Top 10 for LLM Applications identifies critical security risks in applications that integrate Large Language Models. This guide helps developers, security teams, and organizations understand and mitigate these risks.

---

## LLM01:2025 - Prompt Injection

**Description**: Attacks targeting the input layer through human-understandable prompts or nonsensical tokens that cause undesirable model behavior. This has been the #1 risk since the list was first compiled.

**Types**:
- **Direct Injections**: Immediate model failure through malicious prompts
- **Indirect Injections**: Exploitation via external sources (websites, documents, emails)

**Examples**:
- Jailbreaking prompts that bypass safety guidelines
- Hidden instructions in retrieved documents
- Adversarial tokens causing unexpected outputs

**Mitigation Strategies**:
- Constrain model behavior with strict role instructions
- Validate expected output using deterministic code checks
- Implement input sanitization and filtering
- Use prompt injection detection systems

---

## LLM02:2025 - Sensitive Information Disclosure

**Description**: Risk of exposing personal data, health records, financial details, or company secrets through training datasets, RAG systems, or user inputs.

**Examples**:
- Data leakage via targeted prompts
- Unintentional inclusion of PII in training data
- Extraction of proprietary information from fine-tuned models
- RAG system returning confidential documents

**Mitigation Strategies**:
- Integrate data sanitization techniques
- Mask sensitive content in training data
- Enforce robust input and output validation
- Implement data loss prevention (DLP) controls
- Use differential privacy techniques

---

## LLM03:2025 - Supply Chain Vulnerabilities

**Description**: Vulnerabilities in external components including training datasets, LoRA adapters, pre-trained models, and dependencies that may contain backdoors or malicious code.

**Examples**:
- Compromised Python libraries (e.g., malicious packages on PyPI)
- Directly tampered models with hidden parameters
- Backdoored fine-tuning datasets
- Malicious model weights from untrusted sources

**Mitigation Strategies**:
- Use models from verified sources with integrity checks
- Maintain signed Software Bill of Materials (SBOM)
- Scan dependencies for known vulnerabilities
- Verify model provenance and checksums
- Implement secure model deployment pipelines

---

## LLM04:2025 - Data and Model Poisoning

**Description**: Manipulation of data during pre-training, fine-tuning, or embedding processes to introduce vulnerabilities, biases, or backdoors.

**Examples**:
- Injection of biased training examples
- Toxic data in fine-tuning datasets
- Backdoor triggers in training data
- Adversarial poisoning of RAG knowledge bases

**Mitigation Strategies**:
- Track data origins using tools like OWASP CycloneDX
- Rigorously validate data providers against trusted sources
- Implement data quality checks and anomaly detection
- Use adversarial training techniques
- Maintain audit trails for all training data

---

## LLM05:2025 - Improper Output Handling

**Description**: Failure to validate or sanitize LLM outputs before passing them to downstream systems, potentially enabling injection attacks or unintended command execution.

**Examples**:
- SQL injection through hallucinated queries
- Unvalidated outputs to administrative tools
- XSS payloads generated by LLM
- Command injection via code generation

**Mitigation Strategies**:
- Apply context-aware encoding for specific use cases
- Sanitize responses before backend system integration
- Never directly execute LLM-generated code
- Implement output validation schemas
- Use allowlists for acceptable output patterns

---

## LLM06:2025 - Excessive Agency

**Description**: Over-equipping agentic LLMs with excessive functionality, permissions, or autonomy beyond intended use, enabling unauthorized actions.

**Areas of Concern**:
- **Excessive Functionality**: Too many tools/capabilities granted
- **Excessive Permissions**: Overprivileged access to systems
- **Excessive Autonomy**: Actions taken without human oversight

**Examples**:
- Agent with write access to production databases
- Unrestricted API access to financial systems
- Autonomous approval of high-value transactions

**Mitigation Strategies**:
- Limit tool functionality to minimum required
- Enforce minimal access permissions (least privilege)
- Require manual approval for high-impact actions
- Implement action logging and monitoring
- Use human-in-the-loop for sensitive operations

---

## LLM07:2025 - System Prompt Leakage

**Description**: Exposure of sensitive information embedded in system prompts, revealing internal rules, filtering criteria, or credentials.

**Examples**:
- Credential extraction through prompt injection
- Instruction bypass revealing system prompt content
- Reverse engineering of safety guidelines
- Discovery of hidden API keys or configurations

**Mitigation Strategies**:
- Separate sensitive data from system prompts
- Implement independent guardrails to validate outputs
- Use role-based prompt isolation
- Avoid embedding secrets in prompts
- Monitor for prompt extraction attempts

---

## LLM08:2025 - Vector and Embedding Weaknesses

**Description**: Vulnerabilities in RAG systems affecting how vectors and embeddings are generated, stored, or retrieved, allowing content injection or unauthorized data access. With 53% of companies using RAG instead of fine-tuning, this is a critical concern.

**Examples**:
- Unauthorized access to vector databases
- Embedding inversion attacks
- Injection of malicious content into RAG indexes
- Cross-tenant data leakage in shared vector stores

**Mitigation Strategies**:
- Enforce strict access controls with fine-grained permissions
- Validate all data sources before embedding
- Implement tenant isolation in vector databases
- Regular security audits of RAG pipelines
- Use encryption for stored embeddings

---

## LLM09:2025 - Misinformation

**Description**: Generation of false but credible-sounding content through hallucinations, biases, and user over-reliance on unverified outputs.

**Examples**:
- Fabricated package names leading to dependency confusion
- Incorrect medical or legal information
- Hallucinated citations and references
- Confident but wrong technical guidance

**Mitigation Strategies**:
- Use RAG to retrieve verified data from trusted sources
- Encourage cross-verification with authoritative sources
- Implement human fact-checking for critical outputs
- Add confidence indicators to responses
- Train users on limitations of LLM outputs

---

## LLM10:2025 - Unbounded Consumption

**Description**: Uncontrolled resource usage leading to performance degradation, downtime, or unexpected costs through excessive input sizes or repeated requests.

**Examples**:
- Large input submissions overwhelming the system
- High-volume API request attacks (DoS)
- Infinite loops in agentic workflows
- Cost amplification through recursive calls

**Mitigation Strategies**:
- Implement rate limiting and request timeouts
- Dynamically monitor and manage resource allocation
- Set per-user and per-request token limits
- Implement circuit breakers for runaway processes
- Monitor and alert on unusual usage patterns

---

## Quick Reference Matrix

| Risk | Primary Concern | Key Mitigation |
|------|-----------------|----------------|
| LLM01 Prompt Injection | Input manipulation | Input validation, output verification |
| LLM02 Sensitive Info Disclosure | Data leakage | Data sanitization, DLP |
| LLM03 Supply Chain | Compromised components | SBOM, integrity verification |
| LLM04 Data Poisoning | Training manipulation | Data provenance, quality checks |
| LLM05 Improper Output Handling | Injection via output | Output sanitization, encoding |
| LLM06 Excessive Agency | Overprivileged agents | Least privilege, human-in-loop |
| LLM07 System Prompt Leakage | Prompt exposure | Prompt isolation, guardrails |
| LLM08 Vector Weaknesses | RAG vulnerabilities | Access control, encryption |
| LLM09 Misinformation | Hallucinations | RAG, fact-checking, confidence |
| LLM10 Unbounded Consumption | Resource exhaustion | Rate limiting, circuit breakers |

---

## References

- [OWASP LLM Top 10](https://genai.owasp.org/llm-top-10/)
- [OWASP Top 10 for LLM Applications Project](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
- [Confident AI Analysis](https://www.confident-ai.com/blog/owasp-top-10-2025-for-llm-applications-risks-and-mitigation-techniques)

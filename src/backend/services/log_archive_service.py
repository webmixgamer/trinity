"""
Log Archive Service for Trinity Vector Logs.

Handles automatic archival of old log files with compression and optional S3 upload.
"""
import os
import gzip
import shutil
import hashlib
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Dict, Any, List
import asyncio
import logging

from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger

logger = logging.getLogger(__name__)

# Configuration from environment
LOG_RETENTION_DAYS = int(os.getenv("LOG_RETENTION_DAYS", "90"))
LOG_ARCHIVE_ENABLED = os.getenv("LOG_ARCHIVE_ENABLED", "true").lower() == "true"
LOG_CLEANUP_HOUR = int(os.getenv("LOG_CLEANUP_HOUR", "3"))

LOG_DIR = Path("/data/logs")
ARCHIVE_DIR = Path("/data/archives")


class LogArchiveService:
    """Service for archiving old Vector log files."""

    def __init__(self):
        self.scheduler = AsyncIOScheduler()
        self.s3_storage = None
        self._init_s3()

    def _init_s3(self):
        """Initialize S3 storage if enabled."""
        if os.getenv("LOG_S3_ENABLED", "false").lower() == "true":
            try:
                # Import dynamically to avoid dependency if not used
                from .s3_storage import S3ArchiveStorage
                self.s3_storage = S3ArchiveStorage(
                    bucket=os.getenv("LOG_S3_BUCKET", ""),
                    access_key=os.getenv("LOG_S3_ACCESS_KEY", ""),
                    secret_key=os.getenv("LOG_S3_SECRET_KEY", ""),
                    endpoint=os.getenv("LOG_S3_ENDPOINT"),
                    region=os.getenv("LOG_S3_REGION", "us-east-1"),
                )
                logger.info("S3 storage initialized for log archival")
            except Exception as e:
                logger.warning(f"Failed to initialize S3 storage: {e}")
                self.s3_storage = None

    def start(self):
        """Start the archival scheduler."""
        if not LOG_ARCHIVE_ENABLED:
            logger.info("Log archival disabled (LOG_ARCHIVE_ENABLED=false)")
            return

        # Ensure archive directory exists
        ARCHIVE_DIR.mkdir(parents=True, exist_ok=True)

        # Schedule nightly archival
        self.scheduler.add_job(
            self.archive_old_logs,
            CronTrigger(hour=LOG_CLEANUP_HOUR, minute=0),
            id="log_archival",
            name="Nightly Log Archival",
            replace_existing=True,
            misfire_grace_time=3600,  # 1 hour grace period
        )

        self.scheduler.start()
        logger.info(
            f"Log archival scheduler started: Daily at {LOG_CLEANUP_HOUR:02d}:00 UTC "
            f"(retention: {LOG_RETENTION_DAYS} days)"
        )

    def stop(self):
        """Stop the archival scheduler."""
        if self.scheduler.running:
            self.scheduler.shutdown(wait=False)
            logger.info("Log archival scheduler stopped")

    async def archive_old_logs(
        self, retention_days: Optional[int] = None, delete_after_archive: bool = True
    ) -> Dict[str, Any]:
        """
        Archive log files older than retention period.

        Args:
            retention_days: Override default retention days
            delete_after_archive: Delete originals after successful archive

        Returns:
            Dict with archive statistics
        """
        if retention_days is None:
            retention_days = LOG_RETENTION_DAYS

        cutoff_date = datetime.utcnow() - timedelta(days=retention_days)

        if not LOG_DIR.exists():
            logger.warning(f"Log directory not found: {LOG_DIR}")
            return {"error": "Log directory not found"}

        files_processed = 0
        bytes_before = 0
        bytes_after = 0
        s3_uploaded = 0
        errors = []

        # Find all log files older than retention period
        for log_file in LOG_DIR.glob("*.json"):
            try:
                # Extract date from filename: platform-2025-10-06.json or agents-2025-10-06.json
                file_date = self._extract_date_from_filename(log_file.name)

                if file_date and file_date < cutoff_date.date():
                    # Compress the file
                    archive_path = ARCHIVE_DIR / f"{log_file.stem}.json.gz"
                    original_size = log_file.stat().st_size

                    logger.info(f"Archiving {log_file.name} ({original_size / 1024 / 1024:.1f} MB)")

                    # Compress with integrity check
                    if await self._compress_file(log_file, archive_path):
                        compressed_size = archive_path.stat().st_size
                        bytes_before += original_size
                        bytes_after += compressed_size

                        # Upload to S3 if enabled
                        if self.s3_storage:
                            try:
                                await self._upload_to_s3(archive_path, log_file.name)
                                s3_uploaded += 1
                            except Exception as e:
                                logger.error(f"S3 upload failed for {archive_path.name}: {e}")
                                errors.append(f"S3 upload failed: {archive_path.name}")

                        # Delete original if requested
                        if delete_after_archive:
                            log_file.unlink()
                            logger.info(f"Deleted original: {log_file.name}")

                        files_processed += 1
                    else:
                        errors.append(f"Compression failed: {log_file.name}")

            except Exception as e:
                logger.error(f"Error archiving {log_file.name}: {e}")
                errors.append(f"Error: {log_file.name} - {str(e)}")

        # Log summary
        bytes_saved = bytes_before - bytes_after
        logger.info(
            f"Archival complete: {files_processed} files, "
            f"{bytes_saved / 1024 / 1024:.1f} MB saved "
            f"({s3_uploaded} uploaded to S3)"
        )

        return {
            "files_processed": files_processed,
            "bytes_before": bytes_before,
            "bytes_after": bytes_after,
            "bytes_saved": bytes_saved,
            "s3_uploaded": s3_uploaded,
            "errors": errors,
            "retention_days": retention_days,
            "cutoff_date": cutoff_date.isoformat(),
        }

    async def _compress_file(self, source: Path, dest: Path) -> bool:
        """
        Compress a file with gzip and verify integrity.

        Args:
            source: Source file path
            dest: Destination archive path

        Returns:
            True if compression and verification succeeded
        """
        try:
            # Calculate checksum of original
            original_checksum = self._calculate_checksum(source)

            # Compress using gzip level 9
            with open(source, 'rb') as f_in:
                with gzip.open(dest, 'wb', compresslevel=9) as f_out:
                    shutil.copyfileobj(f_in, f_out, length=1024*1024)  # 1MB chunks

            # Verify by decompressing and checking checksum
            with gzip.open(dest, 'rb') as f:
                data = f.read()
                decompressed_checksum = hashlib.sha256(data).hexdigest()

            if original_checksum != decompressed_checksum:
                logger.error(f"Integrity check failed for {dest}")
                dest.unlink()  # Delete corrupt archive
                return False

            return True

        except Exception as e:
            logger.error(f"Compression failed for {source}: {e}")
            if dest.exists():
                dest.unlink()
            return False

    def _calculate_checksum(self, file_path: Path) -> str:
        """Calculate SHA256 checksum of a file."""
        sha256 = hashlib.sha256()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(1024*1024), b''):
                sha256.update(chunk)
        return sha256.hexdigest()

    async def _upload_to_s3(self, archive_path: Path, original_name: str):
        """Upload archive to S3."""
        if not self.s3_storage:
            return

        # Use original name for S3 key (without .gz for consistency)
        s3_key = f"trinity-logs/{original_name}.gz"

        metadata = {
            "original_size": str(archive_path.stat().st_size),
            "archived_at": datetime.utcnow().isoformat(),
            "retention_days": str(LOG_RETENTION_DAYS),
        }

        await self.s3_storage.upload_file(
            file_path=str(archive_path),
            s3_key=s3_key,
            metadata=metadata
        )

    def _extract_date_from_filename(self, filename: str) -> Optional[datetime.date]:
        """
        Extract date from log filename.

        Expected format: platform-2025-10-06.json or agents-2025-10-06.json

        Args:
            filename: Log filename

        Returns:
            datetime.date object or None if parsing fails
        """
        try:
            # Remove extension and split by hyphen
            parts = filename.replace('.json', '').split('-')

            # Last 3 parts should be YYYY-MM-DD
            if len(parts) >= 4:
                year = int(parts[-3])
                month = int(parts[-2])
                day = int(parts[-1])
                return datetime(year, month, day).date()
        except (ValueError, IndexError):
            logger.debug(f"Could not extract date from filename: {filename}")

        return None

    def get_log_stats(self) -> Dict[str, Any]:
        """Get statistics about log files."""
        stats = {
            "log_files": [],
            "archive_files": [],
            "total_log_size": 0,
            "total_archive_size": 0,
            "oldest_log": None,
            "newest_log": None,
        }

        if LOG_DIR.exists():
            log_files = list(LOG_DIR.glob("*.json"))
            for log_file in log_files:
                file_size = log_file.stat().st_size
                file_date = self._extract_date_from_filename(log_file.name)

                stats["log_files"].append({
                    "name": log_file.name,
                    "size": file_size,
                    "date": file_date.isoformat() if file_date else None,
                })
                stats["total_log_size"] += file_size

                if file_date:
                    if not stats["oldest_log"] or file_date < datetime.fromisoformat(stats["oldest_log"]).date():
                        stats["oldest_log"] = file_date.isoformat()
                    if not stats["newest_log"] or file_date > datetime.fromisoformat(stats["newest_log"]).date():
                        stats["newest_log"] = file_date.isoformat()

        if ARCHIVE_DIR.exists():
            archive_files = list(ARCHIVE_DIR.glob("*.json.gz"))
            for archive_file in archive_files:
                file_size = archive_file.stat().st_size
                stats["archive_files"].append({
                    "name": archive_file.name,
                    "size": file_size,
                })
                stats["total_archive_size"] += file_size

        return stats


# Global instance
log_archive_service = LogArchiveService()

